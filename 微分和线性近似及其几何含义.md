# 微分用于线性近似即其几何含义

## 1 一元微分

### 1.1 导数的定义

设 $f(x)$ 是一个定义在某开区间上的实值函数，且在点 $x_0$ 处可导。其导数 $f'(x_0)$ 定义为极限形式：

$$
f'(x_0) = \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}:=\frac{dy}{dx}
$$

此定义描述了函数在 $x_0$ 处的局部变化率

### 1.2 函数局部的行为表征(函数的变化量)

因此在充分小的增量 $h$ 下，函数 $f(x)$ 在 $x_0$ 处的增量可表示为： $dy = f'(x_0) \cdot dx$ 
在局部范围内(同时根据导数的极限定义反推)

$$
\Delta y = f'(x_0) h + o(h)
$$

$$
\Delta y = \underbrace{f'(x_0) \cdot \Delta x}_{\text{线性主部}} + \underbrace{o(\Delta x)}_{\text{高阶无穷小项}}
$$

当局部范围认为足够小时候，我们忽略高阶项，得到函数局部范围的线性近似:

$$
\Delta y \approx f'(x_0) \Delta x
$$

等价的表述为:

$$
f(x) - f(x_0) = f'(x_0) (x - x_0)
$$

$$
f(x + h) - f(x) = f'(x) h
$$

该线性近似可以用于"线性"的表征这个函数的局部行为，而**全域满足该线性近似**的显然是该点的切线方程，即**局部线性近似的全域延拓即为切线方程本身**

故切线方程为:

$$
f(x) = f'(x_0) (x - x_0) + f(x_0)
$$

其中 $x$ 在合理局部范围内是函数的近似表征。

### 1.3 微分的定义

函数在 $x_0$ 处的**微分** $dy$ 定义为线性主部：

$$
dy = f'(x_0) \cdot \Delta x
$$

结合前述增量表达式，我们有：

$$
\Delta y = dy + o(\Delta x) = f'(x_0) \cdot \Delta x + o(\Delta x)
$$

微分 $dy$ 表示函数在 $x_0$ 处沿切线方向的线性增量，而 $o(\Delta x)$ 则代表非线性部分的误差。

### 1.4 线性近似和切线方程

我们通过微来描述函数的局部行为，即
从几何角度，微分对应于函数在 $x_0$ 处的切线上的增量。函数在 $x_0$ 处的切线方程为：

$$
y - f(x_0) = f'(x_0) (x - x_0)
$$

基于此切线方程，我们可以得到函数 $f(x)$ 在 $x_0$ 附近的线性近似：

$$
f(x) \approx f(x_0) + f'(x_0) (x - x_0)
$$

若令 $x = x_0 + h$ （即 $h = x - x_0$ ），则等价形式为：

$$
f(x_0 + h) \approx f(x_0) + f'(x_0) h
$$

然而，这种线性近似仅在 $h$ 充分小时有效，且精度有限，因为它忽略了 $o(\Delta x)$ 中的高阶项。

为了更精确地描述函数在 $x_0$ 附近的局部行为，需要引入高阶导数进行修正。这自然引出了泰勒公式，通过高阶项对函数进行多项式近似，后续分析将进一步展开。

### 1.5 Taylor 公式引入

在线性近似中，我们利用函数在一点的导数（即一阶信息）来估计函数在该点附近的取值。然而，为了更精确地刻画函数的局部行为，我们可以进一步引入函数的高阶导数，构造出**更高阶的近似表达式**。

设函数 $f(x)$ 在某开区间内具有 $n$ 阶连续导数，记 $h = x - x_0$ ，则 $f(x)$ 在点 $x_0$ 附近可展开为：

$$
f(x) = f(x_0) + f'(x_0)h + \frac{f''(x_0)}{2!}h^2 + \cdots + \frac{f^{(n)}(x_0)}{n!}h^n + R_n(h)
$$

其中 $R_n(h)$ 为**余项**，表示高阶近似的误差项，具体形式依赖于所采用的余项形式（如拉格朗日型、皮亚诺型、积分型等）。 
最常见的为**拉格朗日余项**，其形式为：

$$
R_n(h) = \frac{f^{(n+1)}(\xi)}{(n+1)!}h^{n+1}, \quad \text{其中} \ \xi \in (x_0, x)
$$

当 $h \to 0$ 时， $R_n(h) \to 0$ ，因此前 $n$ 项构成函数在 $x_0$ 附近的高阶近似，称为**泰勒多项式（Taylor polynomial）**：

$$
T_n(x) = \sum_{k=0}^{n} \frac{f^{(k)}(x_0)}{k!}(x - x_0)^k
$$

特别地， $n = 1$ 时，Taylor 多项式即为线性近似：

$$
f(x) \approx f(x_0) + f'(x_0)(x - x_0)
$$

这正是前面微分所提供的**一阶逼近**。因此，可以将微分视为 Taylor 展开在一阶截断时所保留的主导部分。

这也恰好是 Lagrange 中值定理的形式:

$$
f(b) - f(a) = f'(\xi)(b - a)
$$

因此这类思想都是用线性量(或更精确一步的使用多项式)去描述函数的局部行为。

## 2 多元函数

### 2.1 偏导数与全微分

#### **偏导数**

对于多元函数 $f: \mathbb{R}^n \to \mathbb{R}$ ，设点 $\mathbf{x}_0 = (x_{01}, x_{02}, \dots, x_{0n})$ 。函数关于第 $i$ 个变量的偏导数定义为：

$$
\frac{\partial f}{\partial x_i}(\mathbf{x}_0) = \lim_{h \to 0} \frac{f(x_{01}, \dots, x_{0i} + h, \dots, x_{0n}) - f(\mathbf{x}_0)}{h}
$$

偏导数表示函数沿 $x_i$ 轴方向的变化率。

**显然，每一个维度增量的叠加即整体增量，即全微分是所有维度的偏微分求和**

#### **全微分**

若 $f$ 在 $\mathbf{x}\_0$ 处对所有变量均可偏导，则当自变量增量 $\Delta \mathbf{x} = (\Delta x_1, \dots, \Delta x_n)$ 很小时，函数增量 $\Delta f = f(\mathbf{x}\_0 + \Delta \mathbf{x}) - f(\mathbf{x}\_0)$ 可近似为：

$$
\Delta f \approx \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x}_0) \Delta x_i
$$

这一线性近似称为**全微分**，记为：

$$
df = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x}_0) \, dx_i
$$

#### **线性近似与几何含义**

全微分提供了函数的线性近似：

$$
f(\mathbf{x}_0 + \Delta \mathbf{x}) \approx f(\mathbf{x}_0) + \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x}_0) \Delta x_i
$$

几何上，这对应于函数图形在 $\mathbf{x}_0$ 处的**切超平面**，类似于一元函数的切线。

### 2.2 方向导数与梯度

#### **方向导数**

多元函数不仅沿坐标轴变化，我们还关心沿任意方向的变化率。设 $\mathbf{u} = (u_1, \dots, u_n)$ 为单位向量（ $\|\mathbf{u}\| = 1$ ），方向导数定义为：

$$
\frac{\partial f}{\partial \mathbf{u}}(\mathbf{x}_0) = \lim_{h \to 0} \frac{f(\mathbf{x}_0 + h \mathbf{u}) - f(\mathbf{x}_0)}{h}
$$

它表示函数沿 $\mathbf{u}$ 方向的变化率。

**现在我们关系函数沿哪个方向的变化最大率最大，即求沿哪个方向的方向导数最大。**

$$
\underset{\|\mathbf{u}\|=1}{\max}\;D_{\mathbf{u}}f(\mathbf{x}_0).
$$

$$
\begin{aligned}
D_{\mathbf{u}}f(\mathbf{x}_0)
&:= \lim_{h\to 0}\frac{f(\mathbf{x}_0 + h\mathbf{u}) - f(\mathbf{x}_0)}{h},
\quad \|\mathbf{u}\|=1 \\
&= \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x}_0)\,u_i \\
&= \nabla f(\mathbf{x}_0)\cdot \mathbf{u} \\
&= \|\nabla f(\mathbf{x}_0)\|\,\|\mathbf{u}\|\,\cos\theta
= \|\nabla f(\mathbf{x}_0)\|\cos\theta,
\end{aligned}
$$

其中 $\theta$ 是 $\nabla f(\mathbf{x}_0)$ 与 $\mathbf{u}$ 之间的夹角。

其中 $x_0 = (x_{01}, x_{02}, \dots, x_{0n})$ 

发现当且仅当 $\vec{u}$ 和 $\nabla f(\vec x_0)$ 同方向时，方向导数达到最大值。

故我们将 $\nabla f(\vec x_0)$ 称为梯度，记为

 $$\nabla f(\vec x_0) = \left(\frac{\partial f}{\partial x_1}(\vec x_0),\dots,\frac{\partial f}{\partial x_n}(\vec x_0)\right)$$ 

**全微分的梯度表达**
对于 $f:\mathbb{R}^n\to\mathbb{R}$ ，若在 $\mathbf{x}_0$ 可微，则

$$
df(\mathbf{x}_0)
= \sum_{i=1}^n \frac{\partial f}{\partial x_i}(\mathbf{x}_0)\,dx_i
= \nabla f(\mathbf{x}_0)\cdot d\mathbf{x}
$$

### 2.3 多元函数的线性近似及切超平面

从一元函数中我们们得到经验，通过线性近似表达局部行为然后延拓到全域可以得到切超平面方程。

 $$df(\mathbf{x}_0) = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x}_0) \, dx_i = \nabla f(\mathbf{x}_0)\cdot d\mathbf{x}$$ 

 $$\Delta f(\mathbf{x}) \approx \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x}_0) \Delta x_i$$ 

 $$f(\vec{x})-f(\vec{x}_0) \approx \sum_{i=1}^{n} \frac{\partial f(\vec{x}_0)}{\partial x_i} (x_i-x_{0i})$$ 

 $$f(\vec{x}) \approx f(\vec{x}_0) + \sum_{i=1}^{n} \frac{\partial f(\vec{x}_0)}{\partial x_i} (x_i-x_{0i})$$ 

故将方程推广到全域即为切超平面方程：

 $$f(\vec{x}) = f(\vec{x}_0) + \sum_{i=1}^{n} \frac{\partial f(\vec{x}_0)}{\partial x_i} (x_i-x_{0i})$$ 

 $$f(\vec{x}) = f(\vec{x}_0) + \nabla f(\vec{x}_0)\cdot (\vec{x}-\vec{x}_0)$$ 

我们得到了线性近似和切超平面方程，但是有的时候我们需要更加精确的近似计算结果，从一元函数得到灵感，我们也可以使用多项式的方式去近似，即多元函数的泰勒展开。

我们显然需要定义出高阶导数,多元函数中应该叫高阶全微分

### 2.4 多元函数的高阶全微分和泰勒展开

#### **高阶全微分**

尝试定义二阶全微分

$$
df(\mathbf{x}_0) = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x}_0) \, dx_i
$$

$$
d(df) = d\left(\sum_{i=1}^{n} \frac{\partial f}{\partial x_i}(\mathbf{x}_0) \, dx_i\right) = \sum_{i=1}^{n} d\left( \frac{\partial f}{\partial x_i}(\mathbf{x}_0) \, dx_i \right)
$$

对于任意项:

 $$d(\frac{\partial f}{\partial x_i}dx_i) = \sum_{j=1}^{n} \frac{\partial^2 f}{\partial x_i \partial x_j}dx_idx_j$$ 

故:

$$
d^2f := d(df) = \sum_{i=1}^{n} \sum_{j=1}^{n} \frac{\partial^2 f}{\partial x_i \partial x_j}dx_idx_j
$$

若二阶偏导数连续有: $\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$ 

介于上述一元函数全微分写成梯度形式(矢量点成),多元函数显然也可以写成类似形式。

**这本质是二重求和的矩阵表达,即二次型:**

$$\begin{aligned}
\sum_{i=1}^n\sum_{j=1}^na_{ij}x_ix_j
\end{aligned}$$

它是由一个求和过程后再求和,单一的求和问题可以转化为两个矩阵的乘积,在对矩阵求和,即转化成三个矩阵乘积.

$$
\begin{aligned}
\sum_{i=1}^n\sum_{j=1}^n a_{ij} x_i x_j 
&= \sum_{i=1}^n \left( x_i (a_{i1} x_1 + a_{i2} x_2 + \cdots + a_{in} x_n) \right) \\
&= \sum_{i=1}^n \left( x_i \begin{pmatrix} a_{i1} & a_{i2} & \cdots & a_{in} \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} \right) \\
&= \begin{pmatrix} x_1 & x_2 & \cdots & x_n \end{pmatrix} 
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} \\
&= \mathbf{x}^T A \mathbf{x}
\end{aligned}
$$


因此二阶全微分的定义为:

$$
d^2f(\mathbf{x}_0) = (d\mathbf{x})^T H_f(\mathbf{x}_0) d\mathbf{x}
$$

其中 $H_f(\mathbf{x}_0)$ 为 $f$ 的二次型，被定义为Hessian矩阵：


$$
H_f(\mathbf{x}_0) = \left[\frac{\partial^2 f}{\partial x_i \partial x_j}(\mathbf{x}_0)\right]_{i,j=1}^n
$$
$$
H_f(\mathbf{x}_0) =
\begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
$$
符号上，有时记作 $H$ 或 $\nabla^2f$ 
**因此，二阶导数可以描述为二次型，沿用线性代数的结论我们可以判定多元函数极值**
多元函数的极值问题->二阶导数的正负->特征值或Sylvester判据


二次型的符号可以由下列方法判定
1. 特征值法
2. Sylvester判据

##### **特征值法**
Hessian矩阵 $H_f(\mathbf{x}_0)$ 是一个对称矩阵，根据线性代数的性质，它可以被正交对角化。也就是说，存在一个正交矩阵 $P$ （满足 $P^T P = I$ ），使得：
 $$ (d\mathbf{x})^T H_f(\mathbf{x}_0) d\mathbf{x} = (d\mathbf{x})^T (P D P^T) d\mathbf{x} = (P^T d\mathbf{x})^T D (P^T d\mathbf{x}) = \mathbf{w}^T D \mathbf{w} $$ 

由于 $D$ 是对角矩阵，展开后有：

 $$ \mathbf{w}^T D \mathbf{w} = \sum_{i=1}^n \lambda_i w_i^2 $$ 

- **正定**：如果所有特征值 $\lambda_i > 0$ ，则对于任意非零向量 $d\mathbf{x}$ ，有 $\sum_{i=1}^n \lambda_i w_i^2 > 0$ ，因此 $H_f(\mathbf{x}_0)$ 是正定的。此时，函数 $f$ 在 $\mathbf{x}_0$ 处有局部极小值。
- **负定**：如果所有特征值 $\lambda_i < 0$ ，则 $\sum_{i=1}^n \lambda_i w_i^2 < 0$ ， $H_f(\mathbf{x}_0)$ 是负定的，函数 $f$ 在 $\mathbf{x}_0$ 处有局部极大值。
- **不定**：如果特征值中既有正值又有负值，则 $\sum_{i=1}^n \lambda_i w_i^2$ 的符号取决于 $\mathbf{w}$ 的选择。例如，可以选择 $w_i \neq 0$ 仅对应正特征值，使和为正；或仅对应负特征值，使和为负。因此， $H_f(\mathbf{x}_0)$ 是不定的， $\mathbf{x}_0$ 是鞍点。
- **半定**：如果特征值中有零，则 $\sum_{i=1}^n \lambda_i w_i^2$ 可能为零或非零，具体取决于 $\mathbf{w}$ 。此时需要进一步分析，但通常不足以确定严格的极值。


##### **Sylvester判据**

对于 $n \times n$ 的对称矩阵 $H_f(\mathbf{x}_0)$ ，其 $k$ 阶主子式 $\Delta_k$ 定义为左上角 $k \times k$ 子矩阵的行列式：

$$ \Delta_k = \det \begin{pmatrix}
h_{11} & h_{12} & \cdots & h_{1k} \\
h_{21} & h_{22} & \cdots & h_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
h_{k1} & h_{k2} & \cdots & h_{kk}
\end{pmatrix}
$$
其中， $h_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}(\mathbf{x}_0)$ 。

- **正定**： $H_f(\mathbf{x}_0)$ 是正定的，当且仅当所有主子式 $\Delta_k > 0$ （ $k = 1, 2, \dots, n$ ）。
- **负定**： $H_f(\mathbf{x}_0)$ 是负定的，当且仅当主子式的符号交替，即 $(-1)^k \Delta_k > 0$ ：
 - $\Delta_1 < 0$ （当 $k=1$ 时）
 - $\Delta_2 > 0$ （当 $k=2$ 时）
 - $\Delta_3 < 0$ （当 $k=3$ 时）
 - 依此类推。
- **其他情况**：如果主子式不满足上述条件，则 $H_f(\mathbf{x}_0)$ 可能是半正定、半负定或不定。

综上多元函数的极值问题转化成 Hessain矩阵的正定性问题,我们可以通过特征值法或Sylvester判据来判断Hessian矩阵的正定性。



#### **多元函数的泰勒展开**

由此我们给出**多元函数的泰勒展开**:

设 $f : \mathbb{R}^n \to \mathbb{R}$ 是在点 $\mathbf{x}_0$ 附近具有足够多阶连续偏导数的函数，记 $d\mathbf{x} = \mathbf{x} - \mathbf{x}_0$ ， 
那么 $f(\mathbf{x})$ 在 $\mathbf{x}_0$ 附近的泰勒展开可以写成：

$$
f(\mathbf{x}_0 + d\mathbf{x}) = f(\mathbf{x}_0) + df(\mathbf{x}_0) + \frac{1}{2!} d^2f(\mathbf{x}_0) + \frac{1}{3!} d^3f(\mathbf{x}_0) + \cdots
$$

特别的,如果只展开到二阶（即忽略三阶及以上小量），那么多元泰勒公式写成全微分形式就是：

$$
f(\mathbf{x}_0 + d\mathbf{x}) = f(\mathbf{x}_0) + df(\mathbf{x}_0) + \frac{1}{2} d^2f(\mathbf{x}_0) + o(\|d\mathbf{x}\|^2)
$$
即：
$$
f(\mathbf{x}_0 + d\mathbf{x}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T d\mathbf{x} + \frac{1}{2} (d\mathbf{x})^T H_f(\mathbf{x}_0) d\mathbf{x} + o(\|d\mathbf{x}\|^2)
$$

以上我们完成了多元函数的微分描述任务 $(\mathbb{R}^n \to \mathbb{R})$ ,更进一步的,我们需要考虑向量值函数的微分描述 $(\mathbb{R}^n \to \mathbb{R}^m)$ 

## 3 向量值函数


向量值函数（或向量函数）是将一个标量或向量输入映射到向量输出的函数。具体定义如下：

### 3.1 定义
设 $\mathbf{x} = [x_1, x_2, \dots, x_m]^T \in \mathbb{R}^m$ 是一个 $m$ 维向量（或标量当 $m=1$ ），向量值函数 $\mathbf{f}$ 定义为：
$$
\mathbf{f}: \mathbb{R}^m \to \mathbb{R}^n
$$
其中，输出是一个 $n$ 维向量：
$$
\mathbf{f}(\mathbf{x}) = \begin{bmatrix}
f_1(x_1, x_2, \dots, x_m) \\
f_2(x_1, x_2, \dots, x_m) \\
\vdots \\
f_n(x_1, x_2, \dots, x_m)
\end{bmatrix}
$$
这里的每个分量 $f_i: \mathbb{R}^m \to \mathbb{R}$ （ $i = 1, 2, \dots, n$ ）是一个标量函数。

所以研究向量值函数的微分本质就是在研究标量函数的微分(即前面的多元函数章节)。

### 3.2 向量值函数的全微分
$$
\begin{aligned}
d\mathbf{f}(\mathbf{x}_0) 
&= 
\begin{pmatrix}
 df_1(\mathbf{x}_0) \\
 df_2(\mathbf{x}_0) \\
 \vdots \\
 df_n(\mathbf{x}_0)
\end{pmatrix} \\
&= 
\begin{pmatrix}
\sum_{i=1}^{m} \frac{\partial f_1}{\partial x_i}(\mathbf{x}_0) \, dx_i \\
\sum_{i=1}^{m} \frac{\partial f_2}{\partial x_i}(\mathbf{x}_0) \, dx_i \\
\vdots \\
\sum_{i=1}^{m} \frac{\partial f_n}{\partial x_i}(\mathbf{x}_0) \, dx_i
\end{pmatrix} \\
&=
\begin{pmatrix}
\frac{\partial f_1}{\partial x_1}(\mathbf{x}_0) & \frac{\partial f_1}{\partial x_2}(\mathbf{x}_0) & \cdots & \frac{\partial f_1}{\partial x_m}(\mathbf{x}_0) \\
\frac{\partial f_2}{\partial x_1}(\mathbf{x}_0) & \frac{\partial f_2}{\partial x_2}(\mathbf{x}_0) & \cdots & \frac{\partial f_2}{\partial x_m}(\mathbf{x}_0) \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1}(\mathbf{x}_0) & \frac{\partial f_n}{\partial x_2}(\mathbf{x}_0) & \cdots & \frac{\partial f_n}{\partial x_m}(\mathbf{x}_0)
\end{pmatrix}
\begin{pmatrix}
dx_1 \\ dx_2 \\ \vdots \\ dx_m
\end{pmatrix} \\
&= J_{\mathbf{f}}(\mathbf{x}_0) \, d\mathbf{x}
\end{aligned}
$$

其中 $J_{\mathbf{f}}(\mathbf{x}_0)$ 为函数 $\mathbf{f}$ 在点 $\mathbf{x}_0$ 处的雅可比矩阵（Jacobian matrix），定义为：

$$
J_{\mathbf{f}}(\mathbf{x}) = \left[ \frac{\partial f_i}{\partial x_j}(\mathbf{x}) \right]_{n \times m}
$$

当 $n = 1$ 时,Jacobian 矩阵退化成
$$
J_{\mathbf{f}}(\mathbf{x}) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_m} \right]
$$

这正是梯度 $\nabla f$ 的转置 $(\nabla f)^T$ 。不过，梯度通常定义为列向量，所以 $J_{\mathbf{f}} = (\nabla f)^T$ 。

**因此:Jacobian矩阵是梯度的向量值形式，即每个分量的梯度构成的矩阵。**
**Jacobian 矩阵是向量值函数的一阶导数矩阵。**
**相对的梯度是Jacobian的标量情形**。

特别的,注意到:

- **Hessian 和 Jacobian**：Hessian 矩阵可以看作是梯度 $\nabla f$ 的 Jacobian 矩阵。计算 $\nabla f$ （一个 $m \times 1$ 向量）的 Jacobian：

$$
J_{\nabla f} = \begin{bmatrix}
\frac{\partial}{\partial x_1} \left( \frac{\partial f}{\partial x_1} \right) & \cdots & \frac{\partial}{\partial x_m} \left( \frac{\partial f}{\partial x_1} \right) \\
\vdots & \ddots & \vdots \\
\frac{\partial}{\partial x_1} \left( \frac{\partial f}{\partial x_m} \right) & \cdots & \frac{\partial}{\partial x_m} \left( \frac{\partial f}{\partial x_m} \right)
\end{bmatrix} = H_f
$$



**Hessian是梯度的梯度(二阶变化量)，梯度是矢量，因此对梯度(矢量)求导(即取Jacobian)即得Hessian矩阵。**

**因此， $H_f = J_{\nabla f}$ ，Hessian 是梯度的一阶导数矩阵。**
 $H_f = J_{\nabla f}$ 的数学本质在于，Hessian 矩阵是梯度 $\nabla f$ 的 Jacobian 矩阵，**体现了二阶导数对一阶导数的依赖关系**。它不仅提供了函数的二阶变化信息，还在几何、优化和物理学中具有重要意义，反映了函数的局部性质和行为。

### 3.3 向量值函数的线性近似

通俗地， $J_{\mathbf{f}}$ 在每一点给出了 $\mathbf{f}$ 的线性近似。例如，当 $\mathbf{f}$ 在某点 $p$ 可微时， $J_{\mathbf{f}}(p)$ 就是 $\mathbf{f}$ 在该点的导数，即在该点的最佳线性逼近。换言之，当 $\mathbf{x}$ 足够接近 $p$ 时，有
$$
\mathbf{f}(\mathbf{x}) \approx \mathbf{f}(p) + J_{\mathbf{f}}(p)\cdot (\mathbf{x}-p).
$$

### 3.4 向量值函数的泰勒展开



为了在向量值函数中引入高阶近似，我们同样对标量情形的泰勒展开进行推广。设 
$$
\mathbf{f}:\mathbb{R}^m\to\mathbb{R}^n,\quad
\mathbf{x}_0\in\mathbb{R}^m,\quad
\mathbf{h} = \mathbf{x}-\mathbf{x}_0
$$ 
当 $\|\mathbf{h}\|$ 足够小时， $\mathbf{f}(\mathbf{x}_0+\mathbf{h})$ 可展开为：

$$\boxed{\mathbf{f}(\mathbf{x}_0+\mathbf{h})=\mathbf{f}(\mathbf{x}_0)+\underbrace{J_\mathbf{f}(\mathbf{x}_0)\mathbf{h}}_\text{一阶线性项}+\underbrace{\frac{1}{2}
\begin{pmatrix}
\mathbf{h}^TH_{f_1}(\mathbf{x}_0)\mathbf{h} \\
\mathbf{h}^TH_{f_2}(\mathbf{x}_0)\mathbf{h} \\
\vdots \\
\mathbf{h}^TH_{f_n}(\mathbf{x}_0)\mathbf{h}
\end{pmatrix}}_\text{二阶二次型项}+o(||\mathbf{h}||^2)}$$

- **常数项** $\mathbf{f}(\mathbf{x}_0)$ ：函数在展开点的原始值。 
- **一阶线性项** $J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}$ ：由雅可比矩阵给出，对应切平面。 
- **二阶二次型项**：每个分量 $f_i$ 的二阶导数构成 Hessian 矩阵 $H_{f_i}$ ，并通过 $\mathbf{h}^T H_{f_i}\mathbf{h}$ 给出二阶校正。将所有分量组合，即得到上述向量形式。 
- **余项** $o(\|\mathbf{h}\|^2)$ ：高于二阶的小量。

##### 张量形式

我们也可用三阶张量 $\mathcal{H}$ 一次性描述所有分量的二阶导数：令

$$
\mathcal{H}\in\mathbb{R}^{n\times m\times m},\quad
\mathcal{H}_{i,j,k} = \frac{\partial^2 f_i}{\partial x_j\partial x_k}(\mathbf{x}_0)
$$

则二阶项可写作：

$$
\bigl(\mathcal{H}[\mathbf{h},\mathbf{h}]\bigr)_i
= \sum_{j,k=1}^m \mathcal{H}_{i,j,k}\,h_j\,h_k
$$

整体展开为：

 $$\mathbf{f}(\mathbf{x}_0+\mathbf{h})=\mathbf{f}(\mathbf{x}_0)+J_\mathbf{f}(\mathbf{x}_0)\mathbf{h}+\frac{1}{2}\mathcal{H}[\mathbf{h},\mathbf{h}]+o(\|\mathbf{h}\|^2)$$ 

这种张量写法在多维高阶分析和自动微分中十分常见。


综上所述:


$$\begin{aligned}
 & f{:}R\to R,df=f^{^{\prime}}(x)dx,d^{2}f=f^{^{\prime\prime}}(x)dx^{2} \\
 & f{:}R^{n}\to R,df=\nabla f(x_{0})\cdot dx,d^{2}f=dx^{T}H_{f}(x_{0})dx \\
 & \left.f{:}R^{n}\to R^{m},df=J_{f}(x_{0})dx,d^{2}f=\left(
\begin{array}
{c}dx^{T}H_{f_{1}}(x_{0})dx \\
dx^{T}H_{f_{2}}(x_{0})dx \\
\vdots \\
dx^{T}H_{f_{m}}(x_{0})dx
\end{array}\right.\right)
\end{aligned}$$


## 4 线性变换与微分

### 4.1 线性变换
#### **定义**
- **加法封闭性**(可加性 additivity)： $\vec{u}, \vec{v} \in V$ ，
 $$
 T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v})
 $$
- **数乘封闭性**(齐次性 Homogeneity)：对于任意标量 $c$ 和向量 $\vec{u} \in V$ ，
 $$
 T(c \vec{u}) = c T(\vec{u})
 $$

结合上述性质，线性变换可以写为：
$$
T(c_1 \vec{u} + c_2 \vec{v}) = c_1 T(\vec{u}) + c_2 T(\vec{v})
$$

**原空间中任意矢量空一写成基向量的线性，而线性变换又具有线性性，这使得变换后的矢量仍然可以使用原始空间的基矢量的线性组合表示，即`线性组合的结构性得以保持`**，具体表现为

 $$T(\sum_{i = 1}^{n}x_ie_i) = \sum_{i = 1}^{n}T(x_ie_i) = \sum_{i = 1}^{n}x_iT(e_i) $$ 
而这种线性组合问题恰巧可以使用矩阵语言来描述:
在有限维向量空间（如 $\mathbb{R}^n \to \mathbb{R}^m$ ）中，线性变换可以用矩阵表示。

#### 矩阵表达


你已经很好地阐述了线性变换的定义和其与矩阵表示之间的联系。以下是继续向后推广，用**矩阵语言**更系统地表达线性变换的内容：


#### **矩阵表示**

设线性变换 $T : \mathbb{R}^n \to \mathbb{R}^m$ ，且 $\{e_1, e_2, \dots, e_n\}$ 是 $\mathbb{R}^n$ 的标准基。由线性性可知：

$$
T(\vec{x}) = T\left(\sum_{i=1}^n x_i e_i\right) = \sum_{i=1}^n x_i T(e_i)
$$

我们记：

$$
A = \begin{bmatrix} T(e_1) & T(e_2) & \cdots & T(e_n) \end{bmatrix}
\quad \text{（ $m \times n$ 矩阵，每列为 $T$ 作用于基向量的结果）}
$$
具体的
$$
A = \begin{bmatrix}
| & | & & | \\
T(e_1) & T(e_2) & \cdots & T(e_n) \\
| & | & & |
\end{bmatrix}
= 
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}
$$

设 有一向量 $\vec{x} = \{x_1,x_2\cdots x_n\}$ ，则：

$$
T(\vec{x}) = A \vec{x}
$$

这意味着**每一个从 $\mathbb{R}^n$ 到 $\mathbb{R}^m$ 的线性变换都可以唯一对应一个 $m \times n$ 的矩阵**，并且我们可以通过只分析基向量的变化而得到A，从而使线性变换转化为矩阵乘法问题。


---
### 4.1.2 矢量叉积在变换后的表达式

在线性空间 $V=\mathbb{R}^3$ 中，给定线性变换 $T: \mathbb{R}^3 \to \mathbb{R}^3$ ，其矩阵表示为 $A \in \mathbb{R}^{3\times3}$ 。对于任意向量 $\vec{u}, \vec{v} \in \mathbb{R}^3$ ，有：

$$
T(\vec{u}) \times T(\vec{v}) = (A\vec{u}) \times (A\vec{v})
$$

利用行列式与伴随矩阵的性质，可得：

$$
(A\vec{u}) \times (A\vec{v}) = \det(A)\,A^{-T}(\vec{u} \times \vec{v})
$$

其中：

* $\det(A)$ 是矩阵 $A$ 的行列式。
* $A^{-T} = (A^{-1})^T$ 是 $A$ 的逆矩阵的转置，也称为伴随矩阵（adjugate）与 $\det(A)$ 的关系式： $A^{-1} = \frac{1}{\det(A)}\mathrm{adj}(A)$ 。

这一公式体现了：在三维空间中，先做线性变换再取叉积，与先取叉积再做逆转置变换只相差 $\det(A)$ 的缩放因子。

### 4.1.3 面积在变换后的数值

考虑由 $\vec{u}, \vec{v} \in \mathbb{R}^n$ 张成的平行四边形，其原始面积为

$$
\mathcal{A} = \|\vec{u} \times \vec{v}\| \quad(\text{当 } n=3) \quad \text{或} \quad \mathcal{A} = \sqrt{\det([\vec{u},\vec{v}]^T[\vec{u},\vec{v}])} \quad (\text{一般 } n).
$$

施加线性变换 $T$ 后，平行四边形的边变为 $T(\vec{u})$ 和 $T(\vec{v})$ ，其变换后面积为：

$$
\mathcal{A}' = \|T(\vec{u}) \times T(\vec{v})\| = |\det(A)|\,\|\vec{u} \times \vec{v}\| = |\det(A)|\,\mathcal{A}.
$$

更一般地，若考虑 $k$ 维子流形（如 $k$ 维平行体），其体积在变换下增/缩因子均为 $|\det(A)|$ 的相应幂次，特别是二维面积的缩放因子为 $|\det(A)|$ 。

**结论**：

1. 三维叉积在变换下遵循 $T(\vec{u})\times T(\vec{v})=\det(A)A^{-T}(\vec{u}\times\vec{v})$ 。
2. 任何由两向量张成的平行四边形，面积在变换下按 $|\det(A)|$ 缩放。


$T(\vec{u}\times\vec{v}) = ?$

---


### 4.1.2 三个公式的详细推导

我们之前得到以下三个关键公式：

1. **先变换再叉积**：

$$
T(\mathbf{u}) \times T(\mathbf{v}) = \det(A)\,A^{-T}(\mathbf{u} \times \mathbf{v}).
$$

2. **先叉积再变换**：

$$
T(\mathbf{u} \times \mathbf{v}) = \frac{1}{\det(A)}\,A\,A^T\bigl(T(\mathbf{u}) \times T(\mathbf{v})\bigr).
$$

3. **面积缩放**：

$$
\mathcal{A}' = \bigl\|T(\mathbf{u}) \times T(\mathbf{v})\bigr\| = |\det(A)|\,\|\mathbf{u} \times \mathbf{v}\| = |\det(A)|\,\mathcal{A}.
$$

下面逐一推导：

---
#### 一、推导 
$T(\mathbf{u}) \times T(\mathbf{v}) = \det(A), A^{-T}(\mathbf{u} \times \mathbf{v})$ 

1. 记 $\mathbf{u} = (u_1, u_2, u_3)^T$ , $\mathbf{v} = (v_1, v_2, v_3)^T$ ，以及 $A = (a_{ij})$ 。令

$$
(A\mathbf{u})_i = \sum_j a_{ij}u_j, \quad (A\mathbf{v})_i = \sum_k a_{ik}v_k.
$$

2. 利用叉积的分量表示

$$
\bigl(A\mathbf{u} \times A\mathbf{v}\bigr)_i = \sum_{j,k} \varepsilon_{ijk}(A\mathbf{u})_j(A\mathbf{v})_k = \sum_{j,k,\,p,q} \varepsilon_{ijk}a_{jp}a_{kq}u_p v_q,
$$

其中 $\varepsilon_{ijk}$ 是 Levi-Civita 符号。

3. 使用恒等式

$$
\sum_{j,k} \varepsilon_{ijk}a_{jp}a_{kq} = \det(A)\,\bigl(A^{-T}\bigr)_{ir}\,\varepsilon_{rpq},
$$

可得

$$
(A\mathbf{u} \times A\mathbf{v})_i = \det(A)\,(A^{-T}(\mathbf{u} \times \mathbf{v}))_i,
$$

故整个向量关系成立。

---

#### 二、推导 
$T(\mathbf{u} \times \mathbf{v}) = \frac{1}{\det(A)}, A, A^T\bigl(T(\mathbf{u}) \times T(\mathbf{v})\bigr)$ 

从第一公式出发：

$$
T(\mathbf{u}) \times T(\mathbf{v}) = \det(A)\,A^{-T}(\mathbf{u} \times \mathbf{v}).
$$

两端同时左乘 $A^T$ ：

$$
A^T\bigl(T(\mathbf{u}) \times T(\mathbf{v})\bigr) = \det(A)\,A^T A^{-T}(\mathbf{u} \times \mathbf{v}) = \det(A)\,(\mathbf{u} \times \mathbf{v}),
$$

利用 $A^T A^{-T} = I$ 。于是

$$
\mathbf{u} \times \mathbf{v} = \frac{1}{\det(A)}\,A^T\bigl(T(\mathbf{u}) \times T(\mathbf{v})\bigr).
$$

再对两端应用 $T$ （即左乘 $A$ ）：

$$
T(\mathbf{u} \times \mathbf{v}) = A(\mathbf{u} \times \mathbf{v}) = \frac{1}{\det(A)}\,A A^T\bigl(T(\mathbf{u}) \times T(\mathbf{v})\bigr).
$$

---

#### 三、推导面积缩放公式 
$\mathcal{A}' = |\det(A)|, \mathcal{A}$ 

1. 原平行四边形由 $\mathbf{u}, \mathbf{v}$ 张成，面积

$$
\mathcal{A} = \|\mathbf{u} \times \mathbf{v}\|.
$$

2. 变换后两边为 $T(\mathbf{u}) = A\mathbf{u}$ 和 $T(\mathbf{v}) = A\mathbf{v}$ ，其叉积大小

$$
\mathcal{A}' = \|A\mathbf{u} \times A\mathbf{v}\| = \bigl\|\det(A)\,A^{-T}(\mathbf{u} \times \mathbf{v})\bigr\| = |\det(A)|\,\|\mathbf{u} \times \mathbf{v}\| = |\det(A)|\,\mathcal{A}.
$$

注意范数与常数可交换，并用到 $|A^{-T}\mathbf{w}| = |\mathbf{w}|$ 当 $A$ 为正交时，但一般仅需抽出 $|\det(A)|$ 。

---

以上即三个公式的完整推导过程。
